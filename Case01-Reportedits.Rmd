---
title: "Case01-Report"
author: "Jingxuan Liu, Linda Tang, Justin Zhao, Mary Wang, Abbey List, and Jack Lichtenstein"
date: "`r Sys.Date()`"
output:
  pdf_document: default
---

```{r load-library}
library(tidyverse)
library(here)
theme_set(theme_light()) # setting a theme
```

```{r load-data}
train <- read.csv(here("data", "data-train.csv"))
test <- read.csv(here("data", "data-test.csv"))
```

## Introduction 

## Methods

Before we begin any analysis, let's further split our training data into a smaller train and test set. We also create cross-validation folds with $K=5$. We do this in an effort to reduce the likelihood of overfitting to the full training data.

```{r cv folds}
# split into train and test
set.seed(123)
spl <- rsample::initial_split(train, prop = 0.8)
tr <- rsample::training(spl)
te <- rsample::testing(spl)

# create folds
set.seed(234)
folds <- rsample::vfold_cv(tr, v = 5)
```

Below we visualize a histogram of the three numerical predictors in these data, `Fr`, `Re`, and `St`. We notice that while the predictors `Fr` and `Re` seem continuous, they are more like factor levels than numeric. As such, we convert both variables to factors.

```{r hist pred, echo=FALSE}
# histograms of predictors
tr %>%
  pivot_longer(cols = c(St:Fr),
               names_to = "metric",
               values_to = "value") %>%
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~ metric, scales = "free")
```

```{r factors, echo=FALSE}
# make Fr, Re factors
train <- train %>%
  mutate(across(c(Fr, Re), factor))
```

Below, we plot the relationship between the three predictor variables and each of the four response variables. Note, the curves fit to the points are via local polynomial regression.

```{r eda plots, echo=FALSE}
tr_long <- tr %>%
  pivot_longer(cols = starts_with("R_moment"),
               names_to = "moment",
               names_prefix = "R_moment_",
               values_to = "value") 

# plot St vs. R_moment_*, color by interaction between Fr, Re
tr_long %>%
  ggplot(aes(sqrt(St), value, color = interaction(Fr, Re))) +
  geom_point() +
  geom_smooth(method="lm") +
  facet_wrap(~ moment, scales = "free", labeller = label_both)

plot_moment <- function(mt) {
  tr_long %>%
    filter(moment == mt) %>%
    ggplot(aes(sqrt(St), value)) +
    geom_point() +
    geom_smooth(method="lm") +
    facet_wrap(~ interaction(Fr, Re), scales = "free") +
    labs(title = paste0("Moment: ", mt))
}

cowplot::plot_grid(plot_moment("1"), plot_moment("2"), 
                   nrow = 1, ncol = 2)
cowplot::plot_grid(plot_moment("3"), plot_moment("4"), 
                   nrow = 1, ncol = 2)
```

In general, these seem to be very strong relationships. We also notice that the interaction between `Fr` and `Re` seems to explain a lot of the variance in the response. That is, the relationship between the third variable `St` and the response depends a lot on the specific interaction between `Fr` and `Re`. We will very likely need to include this interaction in any model we build for these data. 

Additionally, we notice that the relationship between `St` and the response may benefit from taking the square root of `St`.

Below, we perform cross validation of a number of candidate models for each moment. Specifically, for each moment, we train a model to predict the moment with the general formula `~ poly(St, degree)*interaction` where `interaction` is the factor interaction between `Fr` and `Re`. We vary the `degree` parameter from 1 to 3. Additionally, we may choose to take the square root of `St` or take the log of the response.

```{r cv lm, echo=FALSE}
# function to train on a given fold, using given degree, predicting given moment
lm_cv <- function(fold, root, degree, moment, sqrt = TRUE, log = TRUE) {
  data <- rsample::analysis(folds$splits[[fold]]) %>%
    dplyr::mutate(target = !!dplyr::sym(paste0("R_moment_", moment))) %>% 
    tidyr::unite(interaction, Fr, Re, sep = ": ", remove = FALSE)
  assess <- rsample::assessment(folds$splits[[fold]]) %>%
    dplyr::mutate(target = !!dplyr::sym(paste0("R_moment_", moment))) %>% 
    tidyr::unite(interaction, Fr, Re, sep = ": ", remove = FALSE)

  if (isTRUE(sqrt)) {
    data <- data %>%
      dplyr::mutate(St = St^(1/root))
    assess <- assess %>%
      dplyr::mutate(St = St^(1/root))
  }

  if (isTRUE(log)) {
    data <- data %>%
      dplyr::mutate(target = log(target))
    assess <- assess
  }

  if (degree == 1) {
    mod <- data %>%
    lm(target ~ poly(St, 2)*interaction, data = .)
  } else {
    mod <- data %>%
    lm(target ~ as.factor(Fr) + St + Re + as.factor(Fr)*St + Re*St + poly(St, 2)*interaction, data = .)
  }

  train_adj_r2 <- broom::glance(mod)$adj.r.squared

  assess <- assess %>%
    dplyr::mutate(pred = predict(mod, ., type = "response") %>% as.numeric())

  if (isTRUE(log)) {
    assess <- assess %>%
      dplyr::mutate(pred = exp(pred))
  }
  return(assess)
}
```

```{r cv perform, echo=FALSE}
# compute CV
cv <- tidyr::crossing(fold = 1:5,
                      root = c(1, 2, 3, 4, 5, 6, 7, 8),
                      moment = 1:4,
                      sqrt = c(TRUE),
                      log = c(FALSE),
                      degree = c(1, 2)) %>%
  dplyr::mutate(cv = purrr::pmap(list(fold, root, moment, sqrt, log, degree),
                                 ~ lm_cv(fold = ..1,
                                         root = ..2,
                                         moment = ..3,
                                         sqrt = ..4,
                                         log = ..5,
                                         degree = ..6))) %>%
  tidyr::unnest(cv)
```

```{r, echo=FALSE}
cv_summarized <- cv %>%
  dplyr::group_by(moment = factor(moment), root, degree, sqrt, log) %>%
  dplyr::summarise(rmse = sqrt(mean((pred-target)^2)),
                   mae = mean(abs(pred-target)),
                   broom::glance(lm(target ~ pred, data = dplyr::cur_data())),
                   .groups = "drop")

cv_summarized %>% 
  filter(!(log == TRUE & degree == 3)) %>% 
  dplyr::mutate(moment = paste0("Moment: ", moment)) %>%
  tidyr::pivot_longer(cols = c(rmse, mae, adj.r.squared)) %>%
  tidyr::unite(type, moment, name, sep = ": ", remove = FALSE) %>%
  ggplot(aes(root, value, color = as.factor(degree))) +
  geom_line() +
  geom_point() +
  facet_wrap(~ type, scales = "free", nrow = 4)
```

```{r cv results, echo=FALSE}
cv_summarized %>% 
  pivot_longer(cols = c(rmse, mae, adj.r.squared)) %>% 
  group_by(moment, name) %>% 
  mutate(best = ifelse(name == "adj.r.squared", max(value), min(value))) %>% 
  ungroup() %>% 
  filter(value == best) %>% 
  select(moment, degree, sqrt, log, name, value) %>% 
  mutate(value = scales::comma(value, accuracy = 0.00001)) %>% 
  kableExtra::kable(format = "markdown")
```

## Results

```{r}
# potential outlers: 82 / 80, 85
model1 <- train %>%
  tidyr::unite(interaction, Fr, Re, sep = ": ", remove = FALSE) %>%
  lm(R_moment_1 ~ poly(St, 1)*interaction, data = .)
summary(model1)
plot(model1)
```

```{r}
# potential outliers: 20, 65
model2 <- train %>%
  mutate(St = St^(1/4)) %>%
  tidyr::unite(interaction, Fr, Re, sep = ": ", remove = FALSE) %>%
  lm(R_moment_2 ~ poly(St, 2)*interaction, data = .)
summary(model2)
plot(model2)
```

```{r}
# potential outliers: 79, 20
model3 <- train %>%
  mutate(St = St^(1/3)) %>%
  tidyr::unite(interaction, Fr, Re, sep = ": ", remove = FALSE) %>%
  lm(R_moment_3 ~ factor(Fr) + St + Re + as.factor(Fr)*St + Re*St + poly(St, 2)*interaction, data = .)
summary(model3)
plot(model3)
```

```{r}
#potential outliers: 79
model4 <- train %>%
  mutate(St = sqrt(St)) %>%
  tidyr::unite(interaction, Fr, Re, sep = ": ", remove = FALSE) %>%
  lm(R_moment_4 ~ poly(St, 2)*interaction, data = .)
summary(model4)
plot(model4)
```

## Discussion

## References

## Appendix 
